{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bEa9OHc1kLxJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: eli5==0.13.0 in /opt/conda/lib/python3.10/site-packages (0.13.0)\n",
            "Requirement already satisfied: attrs>17.1.0 in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (23.1.0)\n",
            "Requirement already satisfied: jinja2>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (3.1.2)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (1.23.5)\n",
            "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (1.11.3)\n",
            "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20 in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (1.2.2)\n",
            "Requirement already satisfied: graphviz in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (0.20.1)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /opt/conda/lib/python3.10/site-packages (from eli5==0.13.0) (0.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2>=3.0.0->eli5==0.13.0) (2.1.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20->eli5==0.13.0) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.20->eli5==0.13.0) (3.2.0)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "fonts-nanum is already the newest version (20180306-3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!pip install eli5==0.13.0\n",
        "\n",
        "# 한글 폰트 사용을 위한 라이브러리입니다.\n",
        "!apt-get install -y fonts-nanum"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "D7sYByaTkMxx"
      },
      "outputs": [],
      "source": [
        "# visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager as fm\n",
        "fe = fm.FontEntry(\n",
        "    fname=r'/usr/share/fonts/truetype/nanum/NanumGothic.ttf', # ttf 파일이 저장되어 있는 경로\n",
        "    name='NanumBarunGothic')                        # 이 폰트의 원하는 이름 설정\n",
        "fm.fontManager.ttflist.insert(0, fe)              # Matplotlib에 폰트 추가\n",
        "plt.rcParams.update({'font.size': 10, 'font.family': 'NanumBarunGothic'}) # 폰트 설정\n",
        "plt.rc('font', family='NanumBarunGothic')\n",
        "import seaborn as sns\n",
        "\n",
        "# utils\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import warnings;warnings.filterwarnings('ignore')\n",
        "import re\n",
        "import math\n",
        "from geopy.geocoders import Nominatim\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor\n",
        "\n",
        "# Model\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from functools import partial\n",
        "import eli5\n",
        "from eli5.sklearn import PermutationImportance\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "4F1FNrFvkOV2"
      },
      "outputs": [],
      "source": [
        "geo_local = Nominatim(user_agent='South Korea')\n",
        "\n",
        "# 주소 -> 경도, 위도 반환하는 함수\n",
        "def get_coordinates(addr):\n",
        "    try:\n",
        "        geo = geo_local.geocode(addr)\n",
        "        x, y = geo.longitude, geo.latitude\n",
        "        return x, y\n",
        "    except:\n",
        "        return np.nan, np.nan\n",
        "\n",
        "# 주어진 행과 위치 데이터의 모든 지점 간의 거리 계산 및 조건을 만족하는 지점의 개수 반환\n",
        "def calculate_distances_vectorized(row, df_loc, distance, alpha=0):\n",
        "    row_coords = np.radians(np.array([row[0], row[1]]))\n",
        "    df_coords = np.radians(df_loc[['좌표Y', '좌표X']].values.T)\n",
        "    distances = np.linalg.norm(row_coords[:, np.newaxis] - df_coords, axis=0) * 6371000  # row_coords 브로드캐스팅\n",
        "    return np.sum((distances + alpha <= distance).astype(int))\n",
        "\n",
        "# 병렬 처리\n",
        "def get_number_of_object(df_main, df_object, distances, alpha=0):\n",
        "    df_main_loc = df_main[['좌표Y', '좌표X']]\n",
        "    df_object_loc = df_object[['좌표Y', '좌표X']]\n",
        "\n",
        "    with ThreadPoolExecutor() as executor:\n",
        "        result = list(executor.map(lambda row_train: calculate_distances_vectorized(row_train, df_object_loc, distances),\n",
        "                                    tqdm(df_main_loc.itertuples(index=False),\n",
        "                                        total=len(df_main_loc), desc='Building Iteration', position=1)))\n",
        "\n",
        "    df = pd.DataFrame(result)\n",
        "    return df\n",
        "\n",
        "# 위도, 경도로 두 지점간의 거리 계산\n",
        "def haversine_distance(lat1, lon1, lat2, lon2):\n",
        "    radius = 6371.0\n",
        "\n",
        "    lat1 = math.radians(lat1)\n",
        "    lon1 = math.radians(lon1)\n",
        "    lat2 = math.radians(lat2)\n",
        "    lon2 = math.radians(lon2)\n",
        "\n",
        "    dlon = lon2 - lon1\n",
        "    dlat = lat2 - lat1\n",
        "    a = math.sin(dlat / 2) ** 2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon / 2) ** 2\n",
        "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
        "\n",
        "    distance = radius * c\n",
        "    return distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "EIYLxACrkP6R"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape :  (1128094, 40)\n"
          ]
        }
      ],
      "source": [
        "# 필요한 데이터를 load 하겠습니다. 경로는 환경에 맞게 지정해주면 됩니다.\n",
        "train_path = 'merge9.csv'\n",
        "df_train = pd.read_csv(train_path)\n",
        "\n",
        "# Train data와 Test data shape은 아래와 같습니다.\n",
        "print('Train data shape : ', df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pqMirxhTkfvX"
      },
      "outputs": [],
      "source": [
        "# 이상치 제거 방법에는 IQR을 이용하겠습니다.\n",
        "def remove_outliers_iqr(dt, column_name):\n",
        "    df = dt.query('is_test == 0')       # train data 내에 있는 이상치만 제거하도록 하겠습니다.\n",
        "    df_test = dt.query('is_test == 1')\n",
        "\n",
        "    Q1 = df[column_name].quantile(0.25)\n",
        "    Q3 = df[column_name].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    df = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
        "\n",
        "    result = pd.concat([df, df_test])   # test data와 다시 합쳐주겠습니다.\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "0r7QD2OqkoTj"
      },
      "outputs": [],
      "source": [
        "# target 기준 이상치 제거\n",
        "df_train = remove_outliers_iqr(df_train, 'target')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "58XH0_MYkpXZ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1052472, 40)\n"
          ]
        }
      ],
      "source": [
        "print(df_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DIJTcHXekwuC"
      },
      "outputs": [],
      "source": [
        "# 데이터 수가 적은 높은 실거래가의 데이터를 복사하여 넣어줌\n",
        "df_high_cases = df_train[df_train['target'] >= 1300000]\n",
        "df_train = pd.concat([df_train, df_high_cases])\n",
        "df_train = pd.concat([df_train, df_high_cases])\n",
        "\n",
        "df_high_cases2 = df_train[df_train['target'] >= 1000000 & (df_train['target'] < 1300000)]\n",
        "df_train = pd.concat([df_train, df_high_cases2])\n",
        "df_train = pd.concat([df_train, df_high_cases2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "BEhMMkkulCLb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(3129600, 40) (9272, 40)\n"
          ]
        }
      ],
      "source": [
        "dt_train = df_train.query('is_test==0').reset_index()\n",
        "dt_test = df_train.query('is_test==1').reset_index()\n",
        "\n",
        "# 이제 is_test 칼럼은 drop해줍니다.\n",
        "dt_train.drop(['is_test'], axis = 1, inplace=True)\n",
        "dt_test.drop(['is_test'], axis = 1, inplace=True)\n",
        "print(dt_train.shape, dt_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "T4KuPWsnlEhw"
      },
      "outputs": [],
      "source": [
        "final_cols = ['도로명_실거래가순위', '전용면적', 'k-복도유형', 'k-단지분류', '계약년',\n",
        "                  '계약월', '동_실거래가순위', '좌표X', '좌표Y', '건축년도',\n",
        "                  '부촌여부', '상위아파트여부', '대장아파트거리', '도로_실거래가순위', '구',\n",
        "                  '주차대수', '인근지하철역개수', '브랜드명', '건물연식', '계약년월일',\n",
        "                  'top아파트거리', 'target','재건축', '기준금리', 'CLI', '자치구별 지하철 승객 수', '공시지가', '아파트전세가격지수', '학군']\n",
        "\n",
        "dt_train = dt_train[final_cols]\n",
        "dt_test = dt_test[final_cols]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "1Q2xx2oflN0u"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "수치형 변수: ['도로명_실거래가순위', '전용면적', '계약년', '계약월', '동_실거래가순위', '좌표X', '좌표Y', '건축년도', '부촌여부', '상위아파트여부', '대장아파트거리', '도로_실거래가순위', '주차대수', '인근지하철역개수', '건물연식', '계약년월일', 'top아파트거리', 'target', '재건축', '기준금리', 'CLI', '자치구별 지하철 승객 수', '공시지가', '아파트전세가격지수', '학군']\n",
            "범주형 변수: ['k-복도유형', 'k-단지분류', '구', '브랜드명']\n"
          ]
        }
      ],
      "source": [
        "# 파생변수 제작으로 추가된 변수들이 존재하기에, 다시한번 연속형과 범주형 칼럼을 분리해주겠습니다.\n",
        "numerical_columns_v2 = []\n",
        "categorical_columns_v2 = []\n",
        "\n",
        "for column in dt_train.columns:\n",
        "    if pd.api.types.is_numeric_dtype(dt_train[column]):\n",
        "        numerical_columns_v2.append(column)\n",
        "    else:\n",
        "        categorical_columns_v2.append(column)\n",
        "\n",
        "print(\"수치형 변수:\", numerical_columns_v2)\n",
        "print(\"범주형 변수:\", categorical_columns_v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "F9y0KClAlpDT"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/4 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'<' not supported between instances of 'str' and 'float'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[12], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m label_encoders[col] \u001b[38;5;241m=\u001b[39m lbl           \u001b[38;5;66;03m# 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdt_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     16\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m label \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m lbl\u001b[38;5;241m.\u001b[39mclasses_: \u001b[38;5;66;03m# unseen label 데이터인 경우\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     lbl\u001b[38;5;241m.\u001b[39mclasses_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(lbl\u001b[38;5;241m.\u001b[39mclasses_, label)\n",
            "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/arraysetops.py:274\u001b[0m, in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis, equal_nan)\u001b[0m\n\u001b[1;32m    272\u001b[0m ar \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masanyarray(ar)\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43m_unique1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mar\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_index\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mequal_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mequal_nan\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unpack_tuple(ret)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# axis was specified and not None\u001b[39;00m\n",
            "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/arraysetops.py:336\u001b[0m, in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts, equal_nan)\u001b[0m\n\u001b[1;32m    334\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar[perm]\n\u001b[1;32m    335\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 336\u001b[0m     \u001b[43mar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    337\u001b[0m     aux \u001b[38;5;241m=\u001b[39m ar\n\u001b[1;32m    338\u001b[0m mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(aux\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mbool_)\n",
            "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'float'"
          ]
        }
      ],
      "source": [
        "# 아래에서 범주형 변수들을 대상으로 레이블인코딩을 진행해 주겠습니다.\n",
        "# 각 변수에 대한 LabelEncoder를 저장할 딕셔너리\n",
        "label_encoders = {}\n",
        "\n",
        "# Implement Label Encoding\n",
        "for col in tqdm( categorical_columns_v2 ):\n",
        "    lbl = LabelEncoder()\n",
        "\n",
        "    # Label-Encoding을 fit\n",
        "    lbl.fit( dt_train[col].astype(str) )\n",
        "    dt_train[col] = lbl.transform(dt_train[col].astype(str))\n",
        "    label_encoders[col] = lbl           # 나중에 후처리를 위해 레이블인코더를 저장해주겠습니다.\n",
        "\n",
        "    # Test 데이터에만 존재하는 새로 출현한 데이터를 신규 클래스로 추가해줍니다.\n",
        "    for label in np.unique(dt_test[col]):\n",
        "      if label not in lbl.classes_: # unseen label 데이터인 경우\n",
        "        lbl.classes_ = np.append(lbl.classes_, label)\n",
        "\n",
        "    dt_test[col] = lbl.transform(dt_test[col].astype(str))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAv5QlmrlrVF"
      },
      "outputs": [],
      "source": [
        "assert dt_train.shape[1] == dt_test.shape[1]          # train/test dataset의 shape이 같은지 확인해주겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rUqKM0cdluzn"
      },
      "outputs": [],
      "source": [
        "def train_valid_split(data_x, data_y, train_idx, valid_idx):\n",
        "    x_train = data_x.iloc[train_idx]\n",
        "    y_train = data_y[train_idx]\n",
        "    x_valid = data_x.iloc[valid_idx]\n",
        "    y_valid = data_y[valid_idx]\n",
        "    return x_train, y_train, x_valid, y_valid\n",
        "\n",
        "# k-fold cross-validation을 통한 모델 평가 함수\n",
        "def evaluate(data_x, data_y, model, random_state=42, n_splits=5, test_x=None):\n",
        "    kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
        "\n",
        "    oof_y = np.zeros(len(data_x))\n",
        "    feature_importances = np.zeros((len(data_x.columns), n_splits))\n",
        "\n",
        "    if test_x is not None:\n",
        "        test_y = np.zeros((len(test_x), n_splits))\n",
        "\n",
        "    for i, (train_index, valid_index) in enumerate(kf.split(data_x, data_y)):\n",
        "        train_x, train_y, valid_x, valid_y = train_valid_split(data_x, data_y, train_index, valid_index)\n",
        "        model.fit(train_x, train_y)\n",
        "\n",
        "        oof_y[valid_index] = model.predict(valid_x) # out-of-fold 예측값\n",
        "        feature_importances[:, i] = model.feature_importances_\n",
        "\n",
        "        if test_x is not None:\n",
        "            test_y[:, i] = model.predict(test_x)\n",
        "\n",
        "        print(f'{i}th-fold Validation Score : ', mean_squared_error(valid_y, oof_y[valid_index], squared=False))\n",
        "\n",
        "    # out-of-fold 예측값에 대한 RMSE score 계산\n",
        "    score = mean_squared_error(data_y, oof_y, squared=False)\n",
        "    print('OOF RMSE Score : ', score)\n",
        "\n",
        "    return (oof_y, np.mean(test_y, axis=1), np.mean(feature_importances, axis=1)) if test_x is not None else (oof_y, np.mean(feature_importances, axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiuqNjbIlxN0"
      },
      "outputs": [],
      "source": [
        "# Target과 독립변수들을 분리해줍니다.\n",
        "X_train_all = dt_train.drop(['target', '계약년월일'], axis=1)\n",
        "y_train_all = dt_train['target']\n",
        "\n",
        "# Custom validation split - 최근 20% 데이터를 validation set으로 나눔\n",
        "dt_train = dt_train.sort_values('계약년월일')\n",
        "cut = int(len(dt_train)*0.8)\n",
        "train_split = dt_train[:cut]\n",
        "valid_split = dt_train[cut:]\n",
        "\n",
        "X_train = train_split.drop(['target', '계약년월일'], axis=1)\n",
        "y_train = train_split['target']\n",
        "X_val = valid_split.drop(['target', '계약년월일'], axis=1)\n",
        "y_val = valid_split['target']\n",
        "\n",
        "# LGBM 파라미터 설정\n",
        "lgb_params = {\n",
        "            'n_estimators': 2048,\n",
        "            'force_col_wise': True\n",
        "            }\n",
        "\n",
        "model = LGBMRegressor(**lgb_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fGh7MF9lyum"
      },
      "outputs": [],
      "source": [
        "# 모델 훈련 및 평가 후 oof 예측값 가져오기\n",
        "# oof_pred, feature_importances = evaluate(X_train.copy(), y_train.copy(), model)\n",
        "\n",
        "# 모델 학습 및 평가\n",
        "model.fit(X_train, y_train)\n",
        "pred = model.predict(X_val)\n",
        "\n",
        "# 회귀 관련 metric을 통해 train/valid의 모델 적합 결과를 관찰합니다.\n",
        "print(f'Validation RMSE: {mean_squared_error(y_val, pred, squared=False)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFPIVRoHl2fU"
      },
      "outputs": [],
      "source": [
        "# 위 feature importance를 시각화해봅니다.\n",
        "importances = pd.Series(model.feature_importances_, index=list(X_train.columns))\n",
        "importances = importances.sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.title(\"Feature Importances\")\n",
        "sns.barplot(x=importances, y=importances.index)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL4BYqKWl6GD"
      },
      "outputs": [],
      "source": [
        "# 전체 train 데이터로 학습된 모델을 저장\n",
        "model.fit(X_train_all, y_train_all)\n",
        "with open('saved_model.pkl', 'wb') as f:\n",
        "    pickle.dump(model, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNrnvzCql7Ps"
      },
      "outputs": [],
      "source": [
        "# Validation dataset에 target과 pred 값을 채워주도록 하겠습니다.\n",
        "X_val['target'] = y_val\n",
        "X_val['pred'] = pred\n",
        "\n",
        "# Squared_error를 계산하는 함수를 정의하겠습니다.\n",
        "def calculate_se(target, pred):\n",
        "    squared_errors = (target - pred) ** 2\n",
        "    return squared_errors\n",
        "\n",
        "# SE 계산\n",
        "squared_errors = calculate_se(X_val['target'], X_val['pred'])\n",
        "X_val['error'] = squared_errors\n",
        "\n",
        "# Error가 큰 순서대로 sorting 해 보겠습니다.\n",
        "X_val_sort = X_val.sort_values(by='error', ascending=False)\n",
        "X_val_sort.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arDJMleel81f"
      },
      "outputs": [],
      "source": [
        "X_val_sort_top100 = X_val.sort_values(by='error', ascending=False).head(100)        # 예측을 잘 하지못한 top 100개의 data\n",
        "X_val_sort_tail100 = X_val.sort_values(by='error', ascending=False).tail(100)       # 예측을 잘한 top 100개의 data\n",
        "\n",
        "# 해석을 위해 레이블인코딩 된 변수를 복원해줍니다.\n",
        "error_top100 = X_val_sort_top100.copy()\n",
        "for column in categorical_columns_v2 :     # 앞서 레이블 인코딩에서 정의했던 categorical_columns_v2 범주형 변수 리스트를 사용합니다.\n",
        "    error_top100[column] = label_encoders[column].inverse_transform(X_val_sort_top100[column])\n",
        "\n",
        "best_top100 = X_val_sort_tail100.copy()\n",
        "for column in categorical_columns_v2 :     # 앞서 레이블 인코딩에서 정의했던 categorical_columns_v2 범주형 변수 리스트를 사용합니다.\n",
        "    best_top100[column] = label_encoders[column].inverse_transform(X_val_sort_tail100[column])\n",
        "\n",
        "display(error_top100.head(1))\n",
        "display(best_top100.head(1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqxNRpn6l_VH"
      },
      "outputs": [],
      "source": [
        "# 저장된 모델을 불러옵니다.\n",
        "with open('saved_model.pkl', 'rb') as f:\n",
        "    model = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43l4sJwHmBD-"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "X_test = dt_test.drop(['target', '계약년월일'], axis=1)\n",
        "\n",
        "# Test dataset에 대한 inference를 진행합니다.\n",
        "real_test_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ge8L0VQgmDOJ"
      },
      "outputs": [],
      "source": [
        "# 앞서 예측한 예측값들을 저장합니다.\n",
        "preds_df = pd.DataFrame(real_test_pred.astype(int), columns=[\"target\"])\n",
        "preds_df.to_csv('output2.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
